{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mastropy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fits\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mastropy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_file\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time as time\n",
    "\n",
    "import torch\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "\n",
    "from mpol import (\n",
    "    coordinates,\n",
    "    crossval,\n",
    "    datasets,\n",
    "    gridding,\n",
    "    images,\n",
    "    fourier,\n",
    "    losses,\n",
    "    precomposed,\n",
    ")\n",
    "\n",
    "from training_func import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "###########################################################################################################################################\n",
    "fname = '../data/visibilities/mock_visibilities_model_star_delta.npz' # path to the .npz file containing the observed visibilities\n",
    "cell_size = 0.03 # arcseconds\n",
    "npix = 128 # number of pixels per image axis\n",
    "learning_rate = 0.3 # learning rate for the optimizer\n",
    "# n_iter = 25 # number of iterations for the optimizer\n",
    "\n",
    "start_from_dirty_image = False # If True, the initial BaseCube image is set to the dirty image, else to the default flat image.\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a multi-channel dataset... for demonstration purposes we'll use\n",
    "# only the central, single channel\n",
    "d = np.load(fname)\n",
    "uu = d[\"uu\"]\n",
    "vv = d[\"vv\"]\n",
    "weight = d[\"weight\"]\n",
    "data = d[\"data\"]\n",
    "data_re = np.real(data)\n",
    "data_im = np.imag(data)\n",
    "\n",
    "# define the image dimensions, making sure they are big enough to fit all\n",
    "# of the expected emission\n",
    "coords = coordinates.GridCoords(cell_size=cell_size, npix=npix)\n",
    "averager = gridding.DataAverager(\n",
    "    coords=coords, uu=uu, vv=vv, weight=weight, data_re=data_re, data_im=data_im\n",
    ")\n",
    "\n",
    "# export to PyTorch dataset\n",
    "dset = averager.to_pytorch_dataset()\n",
    "\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imager = gridding.DirtyImager(\n",
    "    coords=coords, uu=uu, vv=vv, weight=weight, data_re=data_re, data_im=data_im\n",
    ")\n",
    "\n",
    "# Show the dirty image\n",
    "img, beam = imager.get_dirty_image(weighting=\"briggs\", robust=0.0)\n",
    "kw = {\"origin\": \"lower\", \"extent\": imager.coords.img_ext}\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "ax.imshow(np.squeeze(img), **kw)\n",
    "ax.set_title(\"image\")\n",
    "ax.set_xlabel(r\"$\\Delta \\alpha \\cos \\delta$ [${}^{\\prime\\prime}$]\")\n",
    "ax.set_ylabel(r\"$\\Delta \\delta$ [${}^{\\prime\\prime}$]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the dirty beam\n",
    "img, beam = imager.get_dirty_image(weighting=\"briggs\", robust=0.0)\n",
    "kw = {\"origin\": \"lower\", \"extent\": imager.coords.img_ext}\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "ax.imshow(np.squeeze(beam), **kw)\n",
    "ax.set_title(\"beam\")\n",
    "ax.set_xlabel(r\"$\\Delta \\alpha \\cos \\delta$ [${}^{\\prime\\prime}$]\")\n",
    "ax.set_ylabel(r\"$\\Delta \\delta$ [${}^{\\prime\\prime}$]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1)\n",
    "ax.scatter(uu, vv, s=1.5, rasterized=True, linewidths=0.0, c=\"k\")\n",
    "ax.scatter(\n",
    "    -uu, -vv, s=1.5, rasterized=True, linewidths=0.0, c=\"k\"\n",
    ")  # and Hermitian conjugates\n",
    "ax.set_xlabel(r\"$u$ [k$\\lambda$]\")\n",
    "ax.set_ylabel(r\"$v$ [k$\\lambda$]\")\n",
    "ax.set_title(\"original dataset\")\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a radial and azimuthal partition\n",
    "dartboard = datasets.Dartboard(coords=coords)\n",
    "\n",
    "# create cross validator using this \"dartboard\"\n",
    "k = 5\n",
    "cv = crossval.DartboardSplitGridded(dset, k, dartboard=dartboard, seed=42)\n",
    "\n",
    "# ``cv`` is a Python iterator, it will return a ``(train, test)`` pair of ``GriddedDataset``s for each iteration.\n",
    "# Because we'll want to revisit the individual datasets\n",
    "# several times in this tutorial, we're storeing them into a list\n",
    "k_fold_datasets = [(train, test) for (train, test) in cv]\n",
    "\n",
    "# k_fold_datasets[0][0].vis_gridded.shape\n",
    "\n",
    "k_fold_datasets[0][0].ground_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=k, ncols=2, figsize=(4, 10))\n",
    "\n",
    "for i, (train_subset, test_subset) in enumerate(k_fold_datasets):\n",
    "\n",
    "    # train_subset and test_subset are `GriddedDataset`s\n",
    "\n",
    "    train_mask = train_subset.ground_mask[0]\n",
    "    test_mask = test_subset.ground_mask[0]\n",
    "\n",
    "    ax[i, 0].imshow(\n",
    "        train_mask.detach().numpy(),\n",
    "        interpolation=\"none\",\n",
    "        origin=\"lower\",\n",
    "        extent=coords.vis_ext,\n",
    "        cmap=\"GnBu\",\n",
    "    )\n",
    "\n",
    "    ax[i, 1].imshow(\n",
    "        test_mask.detach().numpy(), origin=\"lower\", extent=coords.vis_ext, cmap=\"GnBu\"\n",
    "    )\n",
    "\n",
    "    ax[i, 0].set_ylabel(\"k-fold {:}\".format(i))\n",
    "\n",
    "ax[0, 0].set_title(\"train mask\")\n",
    "ax[0, 1].set_title(\"test mask\")\n",
    "\n",
    "for a in ax.flatten():\n",
    "    a.xaxis.set_ticklabels([])\n",
    "    a.yaxis.set_ticklabels([])\n",
    "\n",
    "fig.subplots_adjust(left=0.15, hspace=0.0, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dset):\n",
    "    model.train(False)\n",
    "    # evaluate test score\n",
    "    vis = model()\n",
    "    loss = losses.nll_gridded(vis, dset)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(config):\n",
    "    \"\"\"\n",
    "    config is a dictionary that should contain ``lr``, ``lambda_sparsity``, ``lambda_TV``, ``epochs``\n",
    "    \"\"\"\n",
    "    test_scores = []\n",
    "\n",
    "    for k_fold, (train_dset, test_dset) in enumerate(k_fold_datasets):\n",
    "\n",
    "        # create a new model and optimizer for this k_fold\n",
    "        rml = precomposed.SimpleNet(coords=coords, nchan=train_dset.nchan)\n",
    "        optimizer = torch.optim.Adam(rml.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "        # train for a while\n",
    "        # train(rml, train_dset, config, optimizer)\n",
    "        loss_tracker = train(rml=rml, dset=train_dset, hyperparams_config=config, optimizer=optimizer)\n",
    "        # evaluate the test metric\n",
    "        test_scores.append(test(rml, test_dset))\n",
    "\n",
    "    # aggregate all test scores and sum to evaluate cross val metric\n",
    "    test_score = np.sum(np.array(test_scores))\n",
    "\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_image(pars):\n",
    "    rml = precomposed.SimpleNet(coords=coords, nchan=dset.nchan)\n",
    "    optimizer = torch.optim.Adam(rml.parameters(), lr=pars[\"lr\"])\n",
    "    writer = SummaryWriter()\n",
    "    # train(rml, dset, pars, optimizer, writer=writer)\n",
    "    loss_tracker = train(rml=rml, dset=dset, hyperparams_config=pars, optimizer=optimizer, writer=writer)\n",
    "    writer.close()\n",
    "\n",
    "    img_ext = rml.coords.img_ext\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(\n",
    "        np.squeeze(rml.icube.sky_cube.detach().numpy()), origin=\"lower\", extent=img_ext\n",
    "    )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = {'lr': 0.5,\n",
    "        'lambda_sparsity': 1.0e-4,\n",
    "        'lambda_TV': 1.0e-4,\n",
    "        'entropy': 1.0e-4,\n",
    "        'prior_intensity': 1.0e-7,\n",
    "        'TSV': 1.0e-6,\n",
    "        \"epochs\": 600}\n",
    "\n",
    "print(\"Cross validation score:\", cross_validate(pars))\n",
    "train_and_image(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# Define missing variables\n",
    "lambda_sparsity_space = np.logspace(-6, -2, 4)\n",
    "lambda_TV_space = np.logspace(-8, -4, 3)\n",
    "# lambda_TV_space = np.array([1.0e-6])\n",
    "entropy_space = np.logspace(-7, -2, 4)\n",
    "TSV_space = np.logspace(-6, -2, 4)\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "\n",
    "# Fix code block\n",
    "cv_dataframe = pd.DataFrame(columns=['lambda_sparsity', 'lambda_TV', 'entropy', 'TSV', 'cross_val_score'])\n",
    "parameter_total = len(lambda_sparsity_space)*len(lambda_TV_space)*len(entropy_space)*len(TSV_space)\n",
    "start_time = time.time()\n",
    "for sparsity_index, lambda_sparsity in enumerate(lambda_sparsity_space):\n",
    "    for TV_index, lambda_TV in enumerate(lambda_TV_space):\n",
    "        for entropy_index, entropy in enumerate(entropy_space):\n",
    "            for TSV_index, TSV in enumerate(TSV_space):\n",
    "                parameter_count=sparsity_index*len(lambda_TV_space)*len(entropy_space)*len(TSV_space)+TV_index*len(entropy_space)*len(TSV_space)+entropy_index*len(TSV_space)+TSV_index+1\n",
    "                # print(f\"[{parameter_count}/{parameter_total}]\")\n",
    "                run_time_start = time.time()\n",
    "                pars = {'lr': 0.5,\n",
    "                        'lambda_sparsity': lambda_sparsity,\n",
    "                        'lambda_TV': lambda_TV,\n",
    "                        'entropy': entropy,\n",
    "                        'prior_intensity': 1.0e-7,\n",
    "                        'TSV': TSV,\n",
    "                        \"epochs\": 600}\n",
    "                cv_score = cross_validate(pars)\n",
    "                print(\"[{}/{}] --> CV: {:.2f}, log_TSV:{:.1e}, log_entropy:{:.1e}, log_TV:{:.1e}, log_sparsity:{:.1e}\".format(parameter_count, parameter_total, cv_score, np.log10(TSV), np.log10(entropy), np.log10(lambda_TV), np.log10(lambda_sparsity)))\n",
    "                cv_dataframe = pd.concat([cv_dataframe, pd.DataFrame({'lambda_sparsity': lambda_sparsity, 'lambda_TV': lambda_TV, 'entropy': entropy, 'TSV': TSV, 'cross_val_score': cv_score}, index=[0])], ignore_index=True)\n",
    "                # train_and_image(pars)\n",
    "                run_time_end = time.time()\n",
    "                predicted_time = (time.time()-start_time)*(parameter_total-parameter_count)/(parameter_count)\n",
    "                # print(\"runtime: {:.1f} seconds \\t remaining time: {:.1f} minutes\\n\".format(run_time_end-run_time_start, predicted_time/60))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken for cross validation:{:.2f} minutes\".format((end_time-start_time)/60))\n",
    "cv_dataframe.to_csv('cv_dataframe.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataframe.to_csv('cv_dataframe.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataframe.sort_values(by='cross_val_score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cv_dataframe['cross_val_score'].idxmin()\n",
    "print(\"lowest cross validation score:\\n\", cv_dataframe.loc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = {'lr': 0.5,\n",
    "        'lambda_sparsity': cv_dataframe['lambda_sparsity'][idx],\n",
    "        'lambda_TV': cv_dataframe['lambda_TV'][idx],\n",
    "        'entropy': cv_dataframe['entropy'][idx],\n",
    "        'prior_intensity': 1.0e-7,\n",
    "        'TSV': cv_dataframe['TSV'][idx],\n",
    "        \"epochs\": 600}\n",
    "\n",
    "train_and_image(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2e' % x)\n",
    "interest_cols = ['lambda_sparsity', 'entropy', 'TSV', 'cross_val_score']\n",
    "df= cv_dataframe\n",
    "df_interest = df[interest_cols]\n",
    "interest_cols = ['lambda_sparsity', 'entropy', 'TSV']\n",
    "print(df_interest.sort_values(by='cross_val_score', ascending=True))\n",
    "\n",
    "# plot a contour plot for each two parameter combination and the cross validation score. \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, col in enumerate(interest_cols):\n",
    "    X = df_interest[interest_cols[i-1]]\n",
    "    Y = df_interest[interest_cols[i]]\n",
    "    Z = df_interest['cross_val_score']\n",
    "    ax[i].tricontourf(X, Y, Z, levels=14, cmap='RdYlBu')\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_xlabel(interest_cols[i-1])\n",
    "    ax[i].set_ylabel(interest_cols[i])\n",
    "    ax[i].set_title('Cross validation score')\n",
    "    fig.colorbar(ax[i].tricontourf(X, Y, Z, levels=14), ax=ax[i], orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
