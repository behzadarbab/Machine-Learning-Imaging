{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'plotting_func' from '/Users/behzadbojnordiarbab/Projects_Local/Chalmers_PhD/Machine-Learning-Imaging/scripts/plotting_func.py'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image, ImageOps, ImageMath\n",
    "from astropy.utils.data import download_file\n",
    "from mpol.__init__ import zenodo_record\n",
    "from mpol import coordinates, gridding, fourier, losses, precomposed, utils\n",
    "from mpol.images import ImageCube\n",
    "import importlib\n",
    "\n",
    "import training_func\n",
    "importlib.reload(training_func)\n",
    "import plotting_func\n",
    "importlib.reload(plotting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "def RML_imager(visibility_file, cell_size, npix, learning_rate, hyperparams_config, start_from_dirty_image=False, learning_rate_dim=None, n_iter_dim=None,):\n",
    "    '''---------------------------------------------------------------------------------------\n",
    "    Performs the RML imaging process using the SimpleNet model on the input visibilities\n",
    "    to produce surface brightness maps of the source in the image plane. Currently uses\n",
    "    only the NLL loss function, without other regularizers.\n",
    "\n",
    "    # TODO:\n",
    "    1. Start the training loop from the dirty image instead of a flat BaseCube. #*DONE*#\n",
    "    2. Implement regularizers in the loss function. #*DONE*#\n",
    "    3. Optimise regularisers to improve the quality of the final image.\n",
    "    4. Use cross-validation to find the best hyperparameters for the RML imaging process.\n",
    "    5. Compare the final image with that from CASA tclean.\n",
    "    6. Add functionality to save the final image cube as a FITS file.\n",
    "    7. Look into how to image multi-channel continuum visibilities into a single channel image.\n",
    "    ----------------------------------------------------------------------------------------'''\n",
    "\n",
    "    # create a directory to store the outputs of the RML imaging process\n",
    "    if not os.path.exists('RML_loop_outputs/'):\n",
    "        os.makedirs('RML_loop_outputs/')\n",
    "\n",
    "    # load the mock visibilities from the .npz file\n",
    "    d = np.load(visibility_file)\n",
    "    uu = d[\"uu\"]\n",
    "    vv = d[\"vv\"]\n",
    "    weight = d[\"weight\"]\n",
    "    data = d[\"data\"]\n",
    "    data_re = np.real(data)\n",
    "    data_im = np.imag(data)\n",
    "    nvis = len(uu)\n",
    "    print(f'Loaded visibilities from {visibility_file}.')\n",
    "    print(f'The dataset has {nvis} visibilities.\\n')\n",
    "\n",
    "    # check if the input cell size Nyquist samples the spatial frequency represented by the maximum u,v value\n",
    "    max_uv = np.max(np.array([uu,vv]))\n",
    "    max_cell_size = utils.get_maximum_cell_size(max_uv)\n",
    "    if cell_size > max_cell_size:\n",
    "        raise ValueError(f'The input cell size ({cell_size} arcseconds) does not Nyquist sample the spatial frequency represented by the maximum u,v value.\\nThe maximum cell_size that will still Nyquist sample the spatial frequency represented by the maximum u,v value is {max_cell_size:.2f} arcseconds).\\nPlease change the cell size to be less than {max_cell_size:.2f} arcseconds.\\n')\n",
    "    else:\n",
    "        print(f'The input cell size ({cell_size} arcseconds) Nyquist samples the spatial frequency represented by the maximum u,v value.\\n(The maximum cell_size that will still Nyquist sample the spatial frequency represented by the maximum u,v value is {max_cell_size:.2f} arcseconds).\\n')\n",
    "\n",
    "    # plot and save the downloaded (u,v) distribution\n",
    "    plotting_func.plot_uv_distribution(uu, vv)\n",
    "\n",
    "    # instantiate the gridcoords object\n",
    "    coords = coordinates.GridCoords(cell_size=cell_size, npix=npix)\n",
    "\n",
    "    # instantiate the dirty imager object\n",
    "    imager = gridding.DirtyImager(\n",
    "        coords=coords,\n",
    "        uu=uu,\n",
    "        vv=vv,\n",
    "        weight=weight,\n",
    "        data_re=data_re,\n",
    "        data_im=data_im,\n",
    "    )\n",
    "\n",
    "    # calculate the dirty image and the beam using Briggs weighting with robust=0.0\n",
    "    img, beam = imager.get_dirty_image(weighting=\"briggs\", robust=0.0)\n",
    "    print(f\"Calculated dirty beam and dirty image using Briggs weighting with robust=0.0.\")\n",
    "\n",
    "    # plot and save the calculated dirty image and dirty beam\n",
    "    chan = 0\n",
    "    plotting_func.plot_dirty(imager, img, beam, chan)\n",
    "\n",
    "    # instantiate the data averager object\n",
    "    averager = gridding.DataAverager(\n",
    "        coords=coords,\n",
    "        uu=uu,\n",
    "        vv=vv,\n",
    "        weight=weight,\n",
    "        data_re=data_re,\n",
    "        data_im=data_im,\n",
    "        )\n",
    "\n",
    "    # convert the gridded visibilities to a pytorch dataset\n",
    "    dset = averager.to_pytorch_dataset()\n",
    "    print('Gridded and converted visibilities to a pytorch dataset.')\n",
    "    print(f\"The dataset has {dset.nchan} channel(s).\\n\") # TODO: Look into how to image multi-channel continuum visibilities into a single channel image.\n",
    "\n",
    "    if start_from_dirty_image:\n",
    "        ###-----------------------------------------------------------------------------------------------------------------###\n",
    "        rml_dim=training_func.seed_from_dirty_image(learning_rate_dim, n_iter_dim, coords, img, dset)\n",
    "        ###-----------------------------------------------------------------------------------------------------------------###\n",
    "\n",
    "    # initialise SimpleNet\n",
    "    rml = precomposed.SimpleNet(coords=coords, nchan=dset.nchan)\n",
    "    print(f\"SimpleNet network initialised.\")\n",
    "\n",
    "    # choose the model image to set as the initial BaseCube\n",
    "    if start_from_dirty_image:\n",
    "        # load the optimised initial model image (BaseCube) from the .pt file\n",
    "        rml.load_state_dict(torch.load(\"RML_loop_outputs/dirty_image_model.pt\"))\n",
    "        print('Using optimised initial model image (loaded from: RML_loop_outputs/dirty_image_model.pt) based on the dirty image as the initial BaseCube image\\n')\n",
    "    else:\n",
    "        # use the default flat initial model image in the BaseCube\n",
    "        print('Starting from the default flat initial model image (BaseCube).\\n')\n",
    "\n",
    "    # Because we want to compute a clean set of gradient values in a later step, we “zero out” any gradients attached to the tensor components so that they aren’t counted twice.\n",
    "    rml.zero_grad()\n",
    "\n",
    "    # instantiate the SGD optimizer\n",
    "    optimizer = torch.optim.Adam(rml.parameters(), lr=learning_rate)\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    start_time = time.time()\n",
    "    ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~###\n",
    "    # Run the training loop\n",
    "    print(\"Starting the optimisation loop with {} iterations...\".format(hyperparams_config[\"epochs\"]))\n",
    "    loss_tracker=training_func.train(hyperparams_config, dset, rml, optimizer)\n",
    "    print(\"loss tracker: \", loss_tracker)\n",
    "    ###~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~###\n",
    "    end_time = time.time()\n",
    "    print('Done.')\n",
    "\n",
    "    # calculate the time taken for the optimisation loop to finish\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"The optimisation loop finished in {elapsed_time:.2f} seconds.\\n\")\n",
    "\n",
    "    # print the initial and final loss values\n",
    "    print(f\"Initial loss: {loss_tracker[0]:.2f}\")\n",
    "    print(f\"Final loss: {loss_tracker[-1]:.2f}\\n\")\n",
    "\n",
    "    # plot the loss per iteration\n",
    "    plotting_func.plot_loss_per_iter(hyperparams_config, loss_tracker)\n",
    "\n",
    "    # detach the model 'sky' image from the computational graph and convert it to a numpy array\n",
    "    img_cube = rml.icube.sky_cube.detach().numpy() # TODO: Add functionality to save this image cube as a FITS file\n",
    "\n",
    "    # plot the final model image after the last iteration\n",
    "    plotting_func.plot_final_image(rml, img_cube)\n",
    "\n",
    "    return img_cube\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters\n",
    "###########################################################################################################################################\n",
    "visibility_file = '../data/visibilities/mock_visibilities_model_star_new.npz' # path to the .npz file containing the observed visibilities\n",
    "cell_size = 0.03 # arcseconds\n",
    "npix = 128 # number of pixels per image axis\n",
    "learning_rate = 0.3 # learning rate for the optimizer\n",
    "# n_iter = 25 # number of iterations for the optimizer\n",
    "\n",
    "# hyperparameters used in the function and the optimizer (set those not being used to 0)\n",
    "hyperparams_config = (\n",
    "    # {\"lambda_sparsity\": 7.0e-05,\n",
    "    # \"lambda_TV\": 0.00,\n",
    "    # \"entropy\": 1e-03,\n",
    "    # \"prior_intensity\": 1.5e-07,\n",
    "    # \"TSV\": 0.00,\n",
    "    # \"epochs\": 1000,\n",
    "    # }\n",
    "    {'lambda_sparsity': 1.e-4,\n",
    "        'lambda_TV': 1.e-4,\n",
    "        'entropy': 1.e-03,\n",
    "        'prior_intensity': 1.0e-07,\n",
    "        'TSV': 1.0e-03,\n",
    "        \"epochs\": 1000,}\n",
    ")\n",
    "\n",
    "start_from_dirty_image = False # If True, the initial BaseCube image is set to the dirty image, else to the default flat image.\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded visibilities from ../data/visibilities/mock_visibilities_model_star_new.npz.\n",
      "The dataset has 325080 visibilities.\n",
      "\n",
      "The input cell size (0.03 arcseconds) Nyquist samples the spatial frequency represented by the maximum u,v value.\n",
      "(The maximum cell_size that will still Nyquist sample the spatial frequency represented by the maximum u,v value is 0.09 arcseconds).\n",
      "\n",
      "(u,v) distribution plot saved to: RML_loop_outputs/uv_distribution.pdf\n",
      "\n",
      "Calculated dirty beam and dirty image using Briggs weighting with robust=0.0.\n",
      "Dirty beam and dirty image plot saved to: RML_loop_outputs/dirty_beam_and_dirty_image.pdf\n",
      "The dirty image contains 11117 negative pixels.\n",
      "\n",
      "Gridded and converted visibilities to a pytorch dataset.\n",
      "The dataset has 1 channel(s).\n",
      "\n",
      "Starting the optimisation loop with 500 iterations to optimise the initial model image (BaseCube)           based on the dirty image...\n",
      "Loss per iteration (for optimising the BaseCube image to the dirty image) plot saved to: RML_loop_outputs/loss_per_iteration_dim.pdf\n",
      "Loss per iteration (for optimising the initial BaseCube to the dirty image) plot saved to: RML_loop_outputs/optimised_input_model_image_based_on_dirty_image.pdf\n",
      "The optimised initial image based on the dirty image contains 0 negative pixels.\n",
      "Optimised initial model image (BaseCube) based on the dirty image saved to: RML_loop_outputs/dirty_image_model.pt\n",
      "\n",
      "SimpleNet network initialised.\n",
      "Using optimised initial model image (loaded from: RML_loop_outputs/dirty_image_model.pt) based on the dirty image as the initial BaseCube image\n",
      "\n",
      "Starting the optimisation loop with 1000 iterations...\n",
      "loss tracker:  [28.026757470566224, 27.34541639491122, 26.73123203348756, 26.136647575174944, 25.519989185870635, 24.845967047150495, 24.086265197297994, 23.22075979599784, 22.239346384662, 21.14403842304891, 19.950474979902634, 18.68798104262862, 17.397317556564975, 16.126162263942128, 14.92363069420563, 13.834819483747431, 12.896208214362698, 12.132587724582866, 11.555447852167816, 11.162560396574042, 10.93871790870789, 10.857767002179978, 10.885844092427766, 10.985492263888315, 11.12002863440004, 11.257406610545779, 11.372924841728716, 11.450475787368827, 11.48241980661351, 11.468444149643855, 11.413854727076284, 11.327690926641218, 11.220931161717465, 11.10493315416274, 10.990161532560409, 10.885212014520501, 10.7961571174881, 10.726288435102756, 10.676303683141233, 10.644831733727946, 10.629078439469186, 10.625425365488946, 10.629925916327714, 10.63872777179413, 10.648407922249387, 10.656227182510106, 10.660288271311165, 10.6595903184583, 10.653973982695327, 10.643973994886933, 10.63062923087297, 10.615265131496882, 10.599282989286005, 10.583972579313201, 10.570375477134814, 10.559202311520371, 10.5507990813602, 10.545167188955313, 10.542012619600268, 10.54083442011054, 10.541014839381717, 10.541909772335421, 10.542936639544669, 10.543634477653578, 10.543699304070195, 10.543002948296332, 10.5415736545952, 10.539555044944091, 10.53715675173212, 10.534610469464235, 10.532137741977165, 10.529925468151868, 10.528105194318883, 10.526740643721572, 10.525827690069459, 10.525305785410616, 10.525076944283489, 10.5250264154741, 10.525042792902276, 10.525035275129127, 10.524940974773799, 10.524728954586687, 10.524398864480172, 10.523975403265085, 10.523496158750513, 10.523005496236053, 10.522545162706319, 10.522147097389173, 10.521829157571728, 10.521594472286104, 10.521433043309418, 10.521326265195285, 10.521252671105023, 10.521191726638726, 10.521127565710604, 10.521050700383363, 10.520958161553633, 10.52085222670645, 10.520737944088134, 10.520621258696577, 10.520507764147915, 10.520401971756508, 10.520306951502748, 10.520224200961621, 10.520153664889602, 10.520093920476679, 10.520042542949671, 10.519996607847865, 10.519953241489999, 10.519910121749733, 10.519865749837763, 10.519819480772265, 10.519771526647524, 10.51972272632375, 10.519674219579844, 10.519627163977328, 10.519582581384485, 10.51954146146914, 10.519503403475474, 10.519467869803682, 10.519434162360895, 10.51940147501349, 10.519369041863794, 10.519336280674649, 10.5193029276282, 10.51926917180211, 10.519235735245912, 10.519203539360127, 10.519172478852205, 10.519141601398665, 10.519111586705346, 10.519083215895442, 10.519055709755428, 10.519028185739856, 10.519000846552181, 10.518974430692388, 10.518948978709346, 10.518923576650097, 10.518897811073039, 10.518872260684478, 10.518847217833653, 10.518822501990917, 10.518798023916647, 10.518773981157162, 10.518750578834771, 10.51872761337626, 10.518704700514492, 10.518681968195507, 10.518659718502976, 10.518637817242258, 10.51861600939537, 10.518594340430477, 10.518572960494884, 10.518551818530922, 10.51853080643174, 10.518509990283729, 10.51848947703743, 10.51846917563677, 10.518448993208168, 10.518429013971133, 10.518409301784176, 10.518389758371828, 10.518370312146011, 10.518351017660754, 10.518331888742377, 10.518312887321773, 10.518294018882235, 10.51827532879652, 10.518256837407298, 10.518238544454121, 10.518220487045529, 10.518202695281424, 10.518185109116597, 10.518167591299013, 10.518150186899488, 10.51813290093733, 10.518115860555191, 10.518099060719525, 10.518082410013058, 10.518065818500395, 10.518049305117682, 10.518032958272162, 10.518016815410277, 10.518000850423723, 10.517984991814387, 10.517969236460278, 10.517953610657882, 10.517938141340718, 10.517922828386535, 10.517907634940808, 10.517892533297182, 10.517877537117622, 10.517862692073665, 10.517847982467142, 10.5178334121729, 10.517818944585022, 10.517804577734703, 10.517790325631966, 10.517776199888209, 10.517762195331464, 10.517748297337075, 10.517734499305966, 10.517720811615181, 10.51770724060324, 10.517693772559005, 10.517680419691995, 10.51766716298091, 10.517654011089727, 10.517640960225135, 10.517628016524608, 10.517615174152441, 10.517602425991335, 10.517589772352803, 10.517577216404229, 10.517564760081902, 10.5175523993134, 10.517540129832954, 10.517527951110454, 10.517515864967018, 10.517503875225236, 10.51749196961551, 10.517480153141973, 10.517468421020535, 10.517456778558563, 10.517445222434304, 10.517433748243551, 10.517422355676885, 10.517411044709426, 10.517399814498653, 10.517388663828838, 10.517377590744621, 10.517366594107997, 10.517355673520344, 10.517344825379867, 10.517334049386482, 10.517323344083492, 10.51731271468533, 10.517302131080648, 10.517291621756357, 10.51728116804322, 10.5172707696742, 10.517260422907373, 10.517250120845073, 10.517239858006322, 10.517229631247805, 10.517219439103455, 10.517209283625634, 10.517199176841473, 10.517189148009152, 10.517179240989032, 10.517169493832338, 10.517159905261698, 10.517150389964685, 10.517140864525912, 10.517131371849354, 10.51712201577543, 10.517112805152887, 10.517103648514812, 10.517094484755676, 10.51708534886158, 10.517076320492125, 10.517067419377073, 10.517058577849914, 10.517049746273866, 10.517040967252942, 10.517032291712589, 10.517023703340254, 10.517015155309087, 10.517006635443732, 10.516998173393961, 10.516989801612132, 10.516981494277807, 10.51697322327453, 10.51696499856931, 10.516956843773695, 10.51694875529553, 10.51694071141986, 10.516932707765552, 10.51692476304691, 10.516916884929367, 10.516909057872201, 10.51690127285431, 10.516893545442391, 10.51688588159098, 10.516878260495053, 10.516870687520257, 10.516863171555213, 10.516855711731171, 10.516848301046291, 10.51684093397777, 10.516833614596688, 10.516826347092, 10.516819126419877, 10.516811946848089, 10.5168048119953, 10.51679772716425, 10.516790686812428, 10.516783689227672, 10.516776739195947, 10.516769834915797, 10.516762969366479, 10.516756152287435, 10.516749375178044, 10.516742643894641, 10.516735954881643, 10.516729305876815, 10.51672269912521, 10.516716134038205, 10.516709609142225, 10.516703125009377, 10.516696680667039, 10.51669027715969, 10.51668391341413, 10.516677590282509, 10.516671306681086, 10.516665057099477, 10.51665885011758, 10.51665267915849, 10.516646547894439, 10.516640453353396, 10.51663439551701, 10.516628375181535, 10.516622390781112, 10.516616442719911, 10.516610531012928, 10.516604654047505, 10.516598813840302, 10.516593008184444, 10.516587237138241, 10.516581501691736, 10.51657579648462, 10.516570128053901, 10.516564491417897, 10.516558889244234, 10.516553318715415, 10.516547780415582, 10.516542274634727, 10.516536799634476, 10.516531356477318, 10.516525944110242, 10.516520562352762, 10.516515210687245, 10.516509890995579, 10.516504604264327, 10.516499334624834, 10.516494102833843, 10.516488895803638, 10.516483717281682, 10.516478567045562, 10.516473441929751, 10.516468341706961, 10.516463267629941, 10.516458218407815, 10.516453191961789, 10.516448189679506, 10.516443211284274, 10.516438254452911, 10.516433322985728, 10.516428423045058, 10.516423537993706, 10.51641869164561, 10.516413880422723, 10.516409112406354, 10.516404383017493, 10.51639968381488, 10.516395008436449, 10.51639035556434, 10.516385727703044, 10.516381126964408, 10.516376553192472, 10.516372003516716, 10.516367475609316, 10.516362977543357, 10.516358504950206, 10.516354058166504, 10.516349632280438, 10.51634522973283, 10.516340852098526, 10.516336501063376, 10.516332175215734, 10.51632787340071, 10.516323593116365, 10.516319335654998, 10.516315101661686, 10.516310890539286, 10.516306701694605, 10.516302534807538, 10.516298391016035, 10.516294270702074, 10.516290166097784, 10.516286089798841, 10.516282032703321, 10.516277995678491, 10.516273981800449, 10.516269988331324, 10.516266014200085, 10.516262061433576, 10.516258129252504, 10.516254216238762, 10.516250325614937, 10.516246453548437, 10.51624260446517, 10.516238773250837, 10.516234958149965, 10.51623116658535, 10.516227392676171, 10.51622363945919, 10.516219903972564, 10.516216188203918, 10.516212490989608, 10.516208811825756, 10.516205151906643, 10.516201509416078, 10.516197886469786, 10.516194281251131, 10.516190693791442, 10.51618712550415, 10.516183574185701, 10.516180038067775, 10.51617652281299, 10.516173023056522, 10.516169541453714, 10.51616607684965, 10.516162628419567, 10.516159197858697, 10.516155783118226, 10.516152385846837, 10.516149004393162, 10.516145641414631, 10.516142297391104, 10.516138963358108, 10.516135646736721, 10.516132346822374, 10.516129065136141, 10.516125796852927, 10.51612254396985, 10.516119307875373, 10.516116086180906, 10.516112878967647, 10.516109688808456, 10.51610651271244, 10.51610335273014, 10.516100206432878, 10.516097081948702, 10.51609397099392, 10.516090864091039, 10.516087776646549, 10.516084709733429, 10.516081653723484, 10.516078608884394, 10.516075579259711, 10.516072565205377, 10.516069563613648, 10.516066573796683, 10.516063599873311, 10.516060642184113, 10.516057693827094, 10.516054766176358, 10.516051843186991, 10.516048939598523, 10.51604605216099, 10.516043167961882, 10.516040305651657, 10.516037453064497, 10.516034613442303, 10.516031788716054, 10.516028974675407, 10.51602617269709, 10.516023385256151, 10.516020608866746, 10.516017845326262, 10.516015095529324, 10.516012357794988, 10.51600963063604, 10.51600691654363, 10.516004215005632, 10.516001526478782, 10.515998852969846, 10.515996183364638, 10.515993531526913, 10.515990888514727, 10.515988259536218, 10.515985640762889, 10.515983032569304, 10.515980437315333, 10.51597785197594, 10.515975278888083, 10.515972716833488, 10.51597016931023, 10.51596763239158, 10.515965098557187, 10.515962583930376, 10.515960076382763, 10.515957581544294, 10.515955097651045, 10.515952622441102, 10.51595015911389, 10.5159477062211, 10.515945262601917, 10.515942831824166, 10.515940409645125, 10.515937997991317, 10.51593560021425, 10.515933225009665, 10.515930834344841, 10.515928462072505, 10.515926109413986, 10.5159237621554, 10.515921419174443, 10.515919086236252, 10.515916766100744, 10.515914455098617, 10.51591214984644, 10.515909855192234, 10.515907576914435, 10.515905300320728, 10.515903047122524, 10.515900782273514, 10.515898544286838, 10.515896304099233, 10.51589408148848, 10.51589186560128, 10.515889656662388, 10.51588746010664, 10.515885270492685, 10.515883088280614, 10.51588091841419, 10.515878754194214, 10.515876602980157, 10.515874455477558, 10.515872321013637, 10.51587020273207, 10.515868075202405, 10.515865968911733, 10.515863866252666, 10.515861774394939, 10.515859691554189, 10.51585761405099, 10.515855544774087, 10.515853485650538, 10.515851432563109, 10.515849389599047, 10.515847353616536, 10.515845340597009, 10.515843312686453, 10.515841302161032, 10.515839299484922, 10.515837308184803, 10.515835322904916, 10.515833342573707, 10.515831370017166, 10.515829406271632, 10.515827448801838, 10.515825498473921, 10.515823559903646, 10.515821625828666, 10.515819706611333, 10.515817792930898, 10.51581587736693, 10.515813975196728, 10.515812085672026, 10.515810199859503, 10.515808316944483, 10.515806441090572, 10.51580457443027, 10.51580271385732, 10.515800857763162, 10.515799013995341, 10.515797175930066, 10.515795362623988, 10.515793519328522, 10.515791708126986, 10.515789897633322, 10.515788095109604, 10.5157863018497, 10.51578451257865, 10.515782727469443, 10.515780950951525, 10.515779181676105, 10.515777416197352, 10.515775664450645, 10.515773916080606, 10.515772171711086, 10.51577043314535, 10.515768705629757, 10.515766982202411, 10.515765266100274, 10.515763556066258, 10.515761851063626, 10.515760153861395, 10.515758461124623, 10.515756777106537, 10.515755120028489, 10.515753426282668, 10.515751767775615, 10.515750108861052, 10.515748453248742, 10.515746807100076, 10.515745166131092, 10.515743526705593, 10.515741893602522, 10.515740270917911, 10.515738649122364, 10.515737043701055, 10.51573543072882, 10.515733828076394, 10.515732234118351, 10.5157306449303, 10.51572906127348, 10.515727484230823, 10.515725911523015, 10.515724344529602, 10.515722784171743, 10.515721252006518, 10.515719681580677, 10.515718145954967, 10.515716610299584, 10.515715074763177, 10.515713547794979, 10.515712027120339, 10.515710506926654, 10.515708990731651, 10.515707486650973, 10.515705981983212, 10.515704498811541, 10.515702994485423, 10.515701512658097, 10.515700030583512, 10.515698558540489, 10.515697087185194, 10.515695623339838, 10.515694163049101, 10.515692707714384, 10.515691257498258, 10.515689823502852, 10.515688381280029, 10.515686944379816, 10.515685515520452, 10.515684095311407, 10.515682674711494, 10.515681257665076, 10.515679848343154, 10.515678441140544, 10.515677037985952, 10.515675645334625, 10.515674267948127, 10.51567286527989, 10.515671490866923, 10.515670113102031, 10.515668741218553, 10.5156673765611, 10.51566601175739, 10.515664651313624, 10.515663299466281, 10.51566194634342, 10.515660610875464, 10.515659277888837, 10.515657931334038, 10.515656605384759, 10.515655284040387, 10.51565396035396, 10.51565264264092, 10.515651331656967, 10.515650020245092, 10.515648713205932, 10.51564741778561, 10.515646122456848, 10.515644831017935, 10.515643543282799, 10.515642261910344, 10.515640985589458, 10.515639712855965, 10.515638443723942, 10.515637179068298, 10.51563592039944, 10.515634669957612, 10.515633418387095, 10.51563216781765, 10.515630926430038, 10.515629690008101, 10.515628452111546, 10.51562722062299, 10.515625993312907, 10.515624766184114, 10.515623551537779, 10.515622349480907, 10.515621121456933, 10.515619921854071, 10.51561871879038, 10.515617517152716, 10.515616324303046, 10.515615131045225, 10.51561393825406, 10.51561275704229, 10.51561157058767, 10.515610410922662, 10.515609232121365, 10.515608060436891, 10.51560689627433, 10.51560574047092, 10.51560458292372, 10.515603424572856, 10.515602272207607, 10.515601124492115, 10.515599975203695, 10.515598845996141, 10.51559771049004, 10.515596567460653, 10.515595444112751, 10.515594318842776, 10.515593196628538, 10.515592079673763, 10.515590962084078, 10.51558984856153, 10.515588740913001, 10.515587647208555, 10.515586539306732, 10.515585439965124, 10.51558434786451, 10.515583260128908, 10.51558217067496, 10.515581085980568, 10.51558000494757, 10.51557892388361, 10.515577854834216, 10.515576809701075, 10.515575721303845, 10.515574660456483, 10.515573611533325, 10.515572552139819, 10.51557149275738, 10.515570443859621, 10.51556939683645, 10.515568343341556, 10.51556730587036, 10.515566266753869, 10.515565256525786, 10.515564205207287, 10.515563177603038, 10.515562165175844, 10.515561140346461, 10.515560117309722, 10.51555910568267, 10.515558093202218, 10.515557073922704, 10.515556072026603, 10.51555506785011, 10.51555408601689, 10.515553071393128, 10.51555208137967, 10.515551098579385, 10.515550104872068, 10.515549120170288, 10.515548143003011, 10.515547158437533, 10.515546176765795, 10.515545211092466, 10.515544230418568, 10.51554327016753, 10.51554232634839, 10.515541352459993, 10.515540391432912, 10.515539448115081, 10.515538497962396, 10.515537539935062, 10.515536587937786, 10.515535646122652, 10.515534698211134, 10.515533757220654, 10.51553282260732, 10.515531918935613, 10.515530970946777, 10.515530034925433, 10.515529125870787, 10.51552820912862, 10.515527281078075, 10.515526356891751, 10.515525445127249, 10.51552453094227, 10.515523608521173, 10.515522725356355, 10.51552179678592, 10.515520908220193, 10.515520000067871, 10.515519109793901, 10.515518213178302, 10.515517325432413, 10.515516436048518, 10.515515546847341, 10.5155146657433, 10.515513793183352, 10.51551290566885, 10.515512031120368, 10.515511158913087, 10.515510289198897, 10.515509419137768, 10.515508554047333, 10.515507695320203, 10.515506838045491, 10.515505972533244, 10.515505122187843, 10.515504266570959, 10.515503417150876, 10.515502566273774, 10.51550172173545, 10.515500874958605, 10.51550004406817, 10.51549920504019, 10.515498367751421, 10.515497534478023, 10.515496707739747, 10.515495875608131, 10.515495043364398, 10.515494219994446, 10.515493390025927, 10.515492597767926, 10.515491758808787, 10.515490945746121, 10.515490134801013, 10.515489328065089, 10.515488518246817, 10.515487706513152, 10.515486900127064, 10.515486094080655, 10.515485318547158, 10.515484499884197, 10.51548370657572, 10.51548291554029, 10.515482127869676, 10.515481337314764, 10.515480546925806, 10.515479761129738, 10.515478973465664, 10.515478219041238, 10.515477423167171, 10.515476658198102, 10.515475894040051, 10.515475127167914, 10.51547435571208, 10.51547358093849, 10.515472806623487, 10.51547203706321, 10.51547127284977, 10.515470581162386, 10.515469763065957, 10.515469022156264, 10.515468284417885, 10.515467532000653, 10.515466781278995, 10.515466036845966, 10.515465289760826, 10.515464533334395, 10.515463779293487, 10.515463057690468, 10.515462298679218, 10.515461588547904, 10.515460842304659, 10.515460119523278, 10.51545939779954, 10.515458677725464, 10.515457953776098, 10.515457222903258, 10.515456488818318, 10.515455761688704, 10.515455047373234, 10.515454366400075, 10.515453612567539, 10.515452911888229, 10.515452207419605, 10.515451496541058, 10.515450792736486, 10.515450090250319, 10.515449379593075, 10.515448670902087, 10.515447980298037, 10.515447287075705, 10.515446577186294, 10.515445888820716, 10.51544519631513, 10.515444508695387, 10.515443818421684, 10.515443130896625, 10.51544244304757, 10.515441794110343, 10.515441086796532, 10.515440415720615, 10.515439741410752, 10.515439068760925, 10.515438393985155, 10.515437715365563, 10.515437040426914, 10.515436371626237, 10.515435749853202, 10.515435043095616, 10.515434392941424, 10.515433741915281, 10.515433075663177, 10.51543241667742, 10.515431764548996, 10.51543110245841, 10.515430435696272, 10.51542980074784, 10.515429131204812, 10.515428481995173, 10.515427869518732, 10.51542719521521, 10.515426566567378, 10.515425928888513, 10.515425283581381, 10.515424645958758, 10.515424008142844, 10.515423361163219, 10.515422744290817, 10.515422099499085, 10.515421465721952, 10.515420838152078, 10.515420211651344, 10.515419584951331, 10.51541896142839, 10.51541834550173, 10.51541773736301, 10.515417108438555, 10.515416495341038, 10.515415886416351, 10.515415264690175, 10.515414647858826, 10.515414039340822, 10.515413417905414, 10.515412830260914, 10.515412225607081, 10.515411620316462, 10.515411017491047, 10.515410424302384, 10.51540982594047, 10.515409215812937, 10.515408606088121, 10.51540801363853, 10.51540740690022, 10.515406892036932, 10.515406255405209, 10.515405646125355, 10.515405089682798, 10.515404509818591, 10.51540391043336, 10.51540331557685, 10.515402735584649, 10.515402149934378, 10.515401545512637, 10.51540100212456, 10.51540040029579, 10.515399844971213, 10.515399273366773, 10.515398685412348, 10.51539812199969, 10.515397563838452, 10.515396986651623, 10.51539639728101, 10.515395831912562, 10.515395276404066, 10.51539469993801, 10.515394130303687, 10.515393580936383, 10.515393012610497, 10.515392457479098, 10.515391905841266, 10.515391341489472, 10.515390792250825, 10.51539023059753, 10.515389704622937, 10.515389127221413, 10.51538859538756, 10.515388037761019, 10.515387497280102, 10.51538695309384, 10.515386401523937, 10.515385866741266, 10.515385316827828, 10.515384774590947, 10.515384268139385, 10.51538370280563, 10.515383180874856, 10.515382644143891, 10.515382108263488, 10.515381578810151, 10.515381037738736, 10.515380521624818, 10.51537998159001, 10.51537945240007, 10.515378925894467, 10.515378402325343, 10.515377879287602, 10.51537736521752, 10.515376851400008, 10.515376325643546, 10.51537581131313, 10.51537529810689, 10.515374773014438, 10.515374261322586, 10.515373739557917, 10.515373268974427]\n",
      "Done.\n",
      "The optimisation loop finished in 3.29 seconds.\n",
      "\n",
      "Initial loss: 28.03\n",
      "Final loss: 10.52\n",
      "\n",
      "Loss per iteration plot saved to: RML_loop_outputs/loss_per_iteration.pdf\n",
      "\n",
      "Maximum likelihood image plot saved to: RML_loop_outputs/maximum_likelihood_image.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function call\n",
    "###########################################################################################################################################\n",
    "img_cube = RML_imager(visibility_file=visibility_file, cell_size=cell_size, npix=npix, learning_rate=learning_rate, hyperparams_config=hyperparams_config, start_from_dirty_image = True, learning_rate_dim=5, n_iter_dim=500)\n",
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
